{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "755867091548f53d",
   "metadata": {},
   "source": [
    "# Alt-Test: How to Justify Replacing Humans by LLMs\n",
    "\n",
    "To run the alt-test, you need two dictionaries: one with human annotations and another with LLM predictions. Then, you should call the `alt_test` function: <br>\n",
    "```python\n",
    "winning_rate, advantage_prob = alt_test(humans_annotations, llm_annotations, scoring_function, epsilon)\n",
    "```\n",
    "\n",
    "The `winning_rate` represents the proportion of humans the LLM \"wins\", and if `winning_rate >= 0.5` the LLM passes the test. <br>\n",
    "The `advantage_prob` estimates the probability that the LLM annotations are as good as or better than a randomly selected human annotator. It should be used to compare LLMs (higher is better). <br>\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- **`humans_annotations`**:\n",
    "  A dictionary of dictionaries where:\n",
    "  - Outer keys represent annotators (annotator ids).\n",
    "  - Inner dictionaries with keys representing instances (instance ids) and values representing annotations.\n",
    "  **Example:**\n",
    "  ```python\n",
    "  {\n",
    "      'annotator1': {'instance1': 'A', 'instance2': 'B'},\n",
    "      'annotator2': {'instance1': 'A', 'instance2': 'C', 'instance3': 'A'}\n",
    "  }\n",
    "  ```\n",
    "\n",
    "- **`llm_annotations`**:\n",
    "  A dictionary where the keys represent instances (instance ids) and the values represent LLM predictions.\n",
    "  **Example:**\n",
    "  ```python\n",
    "  {'instance1': 'A', 'instance2': 'B', 'instance3': 'A'}\n",
    "  ```\n",
    "\n",
    "- **`scoring_function`**:\n",
    "  Specifies how predictions are evaluated. Can be:\n",
    "  - A string: `'accuracy'` or `'neg_rmse'`.\n",
    "  - A custom function: Takes a prediction and a list of annotations as inputs, returning a score.\n",
    "\n",
    "- **`epsilon`**:\n",
    "  A float representing the cost-benefit penalty for the null hypothesis. Suggested values:\n",
    "  - **0.2**: if annotators are experts.\n",
    "  - **0.15**: if annotators are skilled.\n",
    "  - **0.1**: if annotators are crowd-workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:29.616116Z",
     "start_time": "2025-01-18T07:51:29.614151Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_1samp\n",
    "from typing import List, Dict, Any, Callable, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "503928752382339a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:29.796805Z",
     "start_time": "2025-01-18T07:51:29.785348Z"
    }
   },
   "outputs": [],
   "source": [
    "def open_json(file_path: str) -> Dict:\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def by_procedure(p_values: List[float], q: float) -> List[int]:\n",
    "    p_values = np.array(p_values, dtype=float)\n",
    "    m = len(p_values)\n",
    "    sorted_indices = np.argsort(p_values)\n",
    "    sorted_pvals = p_values[sorted_indices]\n",
    "    # Compute the harmonic sum H_m = 1 + 1/2 + ... + 1/m\n",
    "    H_m = np.sum(1.0 / np.arange(1, m + 1))\n",
    "    # Compute the BY thresholds for each rank i\n",
    "    by_thresholds = (np.arange(1, m + 1) / m) * (q / H_m)\n",
    "    max_i = -1\n",
    "    for i in range(m):\n",
    "        if sorted_pvals[i] <= by_thresholds[i]:\n",
    "            max_i = i\n",
    "    if max_i == -1:\n",
    "        return []\n",
    "    rejected_sorted_indices = sorted_indices[:max_i + 1]\n",
    "    return list(rejected_sorted_indices)\n",
    "\n",
    "\n",
    "def accuracy(pred: Any, annotations: List[Any]) -> float:\n",
    "    return float(np.mean([pred == ann for ann in annotations]))\n",
    "\n",
    "\n",
    "def neg_rmse(pred: Union[int, float], annotations: List[Union[int, float]]) -> float:\n",
    "    return -1 * float(np.sqrt(np.mean([(pred - ann) ** 2 for ann in annotations])))\n",
    "\n",
    "\n",
    "def sim(pred: str, annotations: List[str], similarity_func: Callable) -> float:\n",
    "    return float(np.mean([similarity_func(pred, ann) for ann in annotations]))\n",
    "\n",
    "\n",
    "def ttest(indicators, epsilon: float) -> float:\n",
    "    return ttest_1samp(indicators, epsilon, alternative='less').pvalue\n",
    "\n",
    "\n",
    "def alt_test(llm_annotations: Dict[Union[int, str], Any],\n",
    "             humans_annotations: Dict[Union[int, str], Dict[Union[int, str], Any]],\n",
    "             scoring_function: Union[str, Callable] = 'accuracy',\n",
    "             epsilon: float = 0.2,\n",
    "             q_fdr: float = 0.05,\n",
    "             min_humans_per_instance: int = 2,\n",
    "             min_instances_per_human: int = 30):\n",
    "    # prepare alignment scoring function\n",
    "    if isinstance(scoring_function, str):\n",
    "        if scoring_function == 'accuracy':\n",
    "            scoring_function = accuracy\n",
    "        elif scoring_function == 'neg_rmse':\n",
    "            scoring_function = neg_rmse\n",
    "        else:\n",
    "            raise ValueError(\"Unknown scoring function\")\n",
    "    else:\n",
    "        scoring_function = scoring_function\n",
    "\n",
    "    # prepare sets - i_set has humans as keys, h_set has instances as keys\n",
    "    i_set, h_set = {}, {}\n",
    "    for h, anns in humans_annotations.items():\n",
    "        i_set[h] = list(anns.keys())\n",
    "        for i, ann in anns.items():\n",
    "            if i not in h_set:\n",
    "                h_set[i] = []\n",
    "            h_set[i].append(h)\n",
    "\n",
    "    # remove instances with less than min_humans_per_instance\n",
    "    instances_to_keep = {i for i in h_set if len(h_set[i]) >= min_humans_per_instance and i in llm_annotations}\n",
    "    if len(instances_to_keep) < len(h_set):\n",
    "        print(f\"Dropped {len(h_set) - len(instances_to_keep)} instances with less than {min_humans_per_instance} annotators.\")\n",
    "    i_set = {h: [i for i in i_set[h] if i in instances_to_keep] for h in i_set}\n",
    "    h_set = {i: h_set[i] for i in h_set if i in instances_to_keep}\n",
    "\n",
    "    p_values, advantage_probs, humans = [], [], []\n",
    "    for excluded_h in humans_annotations:\n",
    "        llm_indicators = []\n",
    "        excluded_indicators = []\n",
    "        instances = [i for i in i_set[excluded_h] if i in llm_annotations]\n",
    "        if len(instances) < min_instances_per_human:\n",
    "            print(f\"Skipping annotator {excluded_h} with only {len(instances)} instances < {min_instances_per_human}.\")\n",
    "            continue\n",
    "\n",
    "        for i in instances:\n",
    "            human_ann = humans_annotations[excluded_h][i]\n",
    "            llm_ann = llm_annotations[i]\n",
    "            remaining_anns = [humans_annotations[h][i] for h in h_set[i] if h != excluded_h]\n",
    "            human_score = scoring_function(human_ann, remaining_anns)\n",
    "            llm_score = scoring_function(llm_ann, remaining_anns)\n",
    "            llm_indicators.append(1 if llm_score >= human_score else 0)\n",
    "            excluded_indicators.append(1 if human_score >= llm_score else 0)\n",
    "\n",
    "        diff_indicators = [exc_ind - llm_ind for exc_ind, llm_ind in zip(excluded_indicators, llm_indicators)]\n",
    "        p_values.append(ttest(diff_indicators, epsilon))\n",
    "        advantage_probs.append(float(np.mean(llm_indicators)))\n",
    "        humans.append(excluded_h)\n",
    "\n",
    "    rejected_indices = by_procedure(p_values, q_fdr)\n",
    "    advantage_prob = float(np.mean(advantage_probs))\n",
    "    winning_rate = len(rejected_indices) / len(humans)\n",
    "    return winning_rate, advantage_prob"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Example (paper's data)",
   "id": "b2bb4501ecbd58bf"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a12fe085b2fa973",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T08:09:57.352930Z",
     "start_time": "2025-01-18T08:09:53.722335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing wax\n",
      "wax gemini_flash [FAILED]:\tWinning Rate=0.38\tAdvantage Probability=0.69\n",
      "wax gemini_pro [PASSED]:\tWinning Rate=0.50\tAdvantage Probability=0.74\n",
      "wax gpt-4o [PASSED]:\tWinning Rate=0.50\tAdvantage Probability=0.73\n",
      "wax llama-31 [FAILED]:\tWinning Rate=0.00\tAdvantage Probability=0.57\n",
      "wax gpt-4o-mini [FAILED]:\tWinning Rate=0.00\tAdvantage Probability=0.59\n",
      "wax mistral-v03 [FAILED]:\tWinning Rate=0.00\tAdvantage Probability=0.50\n",
      "\n",
      "Computing lgbteen\n",
      "lgbteen gemini_flash [FAILED]:\tWinning Rate=0.25\tAdvantage Probability=0.71\n",
      "lgbteen gemini_pro [FAILED]:\tWinning Rate=0.00\tAdvantage Probability=0.67\n",
      "lgbteen gpt-4o [PASSED]:\tWinning Rate=0.75\tAdvantage Probability=0.77\n",
      "lgbteen llama-31 [FAILED]:\tWinning Rate=0.00\tAdvantage Probability=0.72\n",
      "lgbteen gpt-4o-mini [PASSED]:\tWinning Rate=0.75\tAdvantage Probability=0.76\n",
      "lgbteen mistral-v03 [FAILED]:\tWinning Rate=0.25\tAdvantage Probability=0.75\n",
      "\n",
      "Computing mtbench\n",
      "mtbench gemini_flash [FAILED]:\tWinning Rate=0.00\tAdvantage Probability=0.72\n",
      "mtbench gemini_pro [FAILED]:\tWinning Rate=0.00\tAdvantage Probability=0.76\n",
      "mtbench gpt-4o [FAILED]:\tWinning Rate=0.00\tAdvantage Probability=0.77\n",
      "mtbench llama-31 [FAILED]:\tWinning Rate=0.00\tAdvantage Probability=0.69\n",
      "mtbench gpt-4o-mini [FAILED]:\tWinning Rate=0.00\tAdvantage Probability=0.74\n",
      "mtbench mistral-v03 [FAILED]:\tWinning Rate=0.00\tAdvantage Probability=0.68\n",
      "\n",
      "Computing framing\n",
      "framing gemini_flash [PASSED]:\tWinning Rate=1.00\tAdvantage Probability=0.83\n",
      "framing gemini_pro [PASSED]:\tWinning Rate=1.00\tAdvantage Probability=0.91\n",
      "framing gpt-4o [PASSED]:\tWinning Rate=1.00\tAdvantage Probability=0.92\n",
      "framing llama-31 [PASSED]:\tWinning Rate=0.50\tAdvantage Probability=0.80\n",
      "framing gpt-4o-mini [PASSED]:\tWinning Rate=1.00\tAdvantage Probability=0.87\n",
      "framing mistral-v03 [FAILED]:\tWinning Rate=0.25\tAdvantage Probability=0.80\n",
      "\n",
      "Computing cebab_aspects\n",
      "cebab_aspects gemini_flash [PASSED]:\tWinning Rate=0.70\tAdvantage Probability=0.91\n",
      "cebab_aspects gemini_pro [PASSED]:\tWinning Rate=0.90\tAdvantage Probability=0.94\n",
      "cebab_aspects gpt-4o [PASSED]:\tWinning Rate=0.90\tAdvantage Probability=0.93\n",
      "cebab_aspects llama-31 [PASSED]:\tWinning Rate=0.60\tAdvantage Probability=0.89\n",
      "cebab_aspects gpt-4o-mini [PASSED]:\tWinning Rate=0.50\tAdvantage Probability=0.90\n",
      "cebab_aspects mistral-v03 [FAILED]:\tWinning Rate=0.10\tAdvantage Probability=0.81\n",
      "\n",
      "Computing summeval\n",
      "summeval gemini_flash [FAILED]:\tWinning Rate=0.00\tAdvantage Probability=0.46\n",
      "summeval gemini_pro [FAILED]:\tWinning Rate=0.00\tAdvantage Probability=0.44\n",
      "summeval gpt-4o [FAILED]:\tWinning Rate=0.00\tAdvantage Probability=0.48\n",
      "summeval llama-31 [FAILED]:\tWinning Rate=0.00\tAdvantage Probability=0.58\n",
      "summeval gpt-4o-mini [FAILED]:\tWinning Rate=0.00\tAdvantage Probability=0.54\n",
      "summeval mistral-v03 [FAILED]:\tWinning Rate=0.00\tAdvantage Probability=0.62\n",
      "\n",
      "Computing 10k_prompts\n",
      "10k_prompts gemini_flash [FAILED]:\tWinning Rate=0.31\tAdvantage Probability=0.67\n",
      "10k_prompts gemini_pro [FAILED]:\tWinning Rate=0.08\tAdvantage Probability=0.63\n",
      "10k_prompts gpt-4o [PASSED]:\tWinning Rate=0.69\tAdvantage Probability=0.76\n",
      "10k_prompts llama-31 [FAILED]:\tWinning Rate=0.15\tAdvantage Probability=0.67\n",
      "10k_prompts gpt-4o-mini [PASSED]:\tWinning Rate=0.92\tAdvantage Probability=0.80\n",
      "10k_prompts mistral-v03 [FAILED]:\tWinning Rate=0.15\tAdvantage Probability=0.67\n",
      "\n",
      "Computing cebab_stars\n",
      "cebab_stars gemini_flash [PASSED]:\tWinning Rate=0.60\tAdvantage Probability=0.82\n",
      "cebab_stars gemini_pro [PASSED]:\tWinning Rate=0.80\tAdvantage Probability=0.87\n",
      "cebab_stars gpt-4o [PASSED]:\tWinning Rate=0.90\tAdvantage Probability=0.90\n",
      "cebab_stars llama-31 [PASSED]:\tWinning Rate=0.60\tAdvantage Probability=0.85\n",
      "cebab_stars gpt-4o-mini [PASSED]:\tWinning Rate=0.90\tAdvantage Probability=0.89\n",
      "cebab_stars mistral-v03 [PASSED]:\tWinning Rate=0.50\tAdvantage Probability=0.83\n",
      "\n",
      "Computing lesion\n",
      "lesion gemini_flash [FAILED]:\tWinning Rate=0.17\tAdvantage Probability=0.71\n",
      "lesion gemini_pro [PASSED]:\tWinning Rate=1.00\tAdvantage Probability=0.81\n",
      "lesion gpt-4o [FAILED]:\tWinning Rate=0.00\tAdvantage Probability=0.62\n",
      "lesion gpt-4o-mini [PASSED]:\tWinning Rate=0.67\tAdvantage Probability=0.73\n",
      "\n"
     ]
    }
   ],
   "source": [
    "annotations_folder = \"data/\"\n",
    "datasets_scoring_functions = {\n",
    "    'wax': 'accuracy',\n",
    "    'lgbteen': 'accuracy',\n",
    "    'mtbench': 'accuracy',\n",
    "    'framing': 'accuracy',\n",
    "    'cebab_aspects': 'accuracy',\n",
    "    'summeval': 'neg_rmse',\n",
    "    '10k_prompts': 'neg_rmse',\n",
    "    'cebab_stars': 'neg_rmse',\n",
    "    'lesion': 'neg_rmse',\n",
    "}\n",
    "datasets_epsilons = {\n",
    "    'wax': 0.1,\n",
    "    'lgbteen': 0.2,\n",
    "    'mtbench': 0.2,\n",
    "    'framing': 0.15,\n",
    "    'cebab_aspects': 0.1,\n",
    "    'summeval': 0.2,\n",
    "    '10k_prompts': 0.15,\n",
    "    'cebab_stars': 0.1,\n",
    "    'lesion': 0.15,\n",
    "}\n",
    "\n",
    "for dataset_name, metric in datasets_scoring_functions.items():\n",
    "    epsilon = datasets_epsilons[dataset_name]\n",
    "    print(f\"Computing {dataset_name}\")\n",
    "    humans_annotations = open_json(os.path.join(annotations_folder, dataset_name, 'human_annotations.json'))\n",
    "    llms_annotations = open_json(os.path.join(annotations_folder, dataset_name, 'llm_annotations.json'))\n",
    "\n",
    "    for llm_name, llm_annotations in llms_annotations.items():\n",
    "        wr, ap = alt_test(llm_annotations, humans_annotations, metric, epsilon=epsilon)\n",
    "        print(f\"{dataset_name} {llm_name} [{'PASSED' if wr >= 0.5 else 'FAILED'}]:\\tWinning Rate={wr:.2f}\\tAdvantage Probability={ap:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1efb52941380550",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
